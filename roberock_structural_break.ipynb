{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roberock/structural-break/blob/master/roberock_structural_break.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIm9zgAEEaBy"
      },
      "source": [
        "https://hub.crunchdao.com/competitions/structural-break/submit/notebook?projectName=semantic-penguin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0_OBkYWD8ch",
        "outputId": "bb9b53c3-e197-4d0d-92cb-304e28aee128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crunch-cli, version 7.4.0\n",
            "main.py: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/27380/main.py (21371 bytes)\n",
            "notebook.ipynb: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/27380/notebook.ipynb (42046 bytes)\n",
            "requirements.txt: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/submissions/27380/requirements.original.txt (212 bytes)\n",
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "                                \n",
            "---\n",
            "Success! Your environment has been correctly setup.\n",
            "Next recommended actions:\n",
            "1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n",
            "2. Execute the cells with your code\n",
            "3. Run a test: `crunch.test()`\n",
            "4. Download and submit your code to the platform!\n"
          ]
        }
      ],
      "source": [
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "!crunch setup-notebook structural-break 7zhoh1AuNOkWercet1v9wiWi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xg1A0AMxD9B2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import warnings\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics\n",
        "\n",
        "from scipy.stats import kurtosis, skew\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "# --- Rich features: robust stats + higher moments + FFT + wavelets + Random rubbish\n",
        "from numpy.fft import rfft\n",
        "import pywt  # pip install PyWavelets\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHD6YzwxD9GR",
        "outputId": "fc57589b-2116-4555-ffdb-9489f4045567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 7.4.0\n",
            "available ram: 12.67 gb\n",
            "available cpu: 2 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LEolCTtEBZP",
        "outputId": "b5c78740-eea4-4441-abf6-588c14cd8103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        }
      ],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NlEPNtSENfL"
      },
      "source": [
        "\n",
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vjmsdCQNEQZU"
      },
      "outputs": [],
      "source": [
        "def _stats_robust(x):\n",
        "    if x.size == 0:\n",
        "        return {}\n",
        "    q = np.percentile(x, [1, 5, 10, 25, 35, 50, 65, 75, 90, 95, 99])\n",
        "    q1, q5, q10, q25, q35, q50, q65, q75, q90, q95, q99 = q\n",
        "    mad = np.median(np.abs(x - q50))  # Median Absolute Deviation\n",
        "    iqr = q75 - q25\n",
        "\n",
        "    # basic stats\n",
        "    out = {\n",
        "        \"mean\": float(np.mean(x)), \"std\": float(np.std(x)), \"var\": float(np.var(x)), \"min\": float(np.min(x)), \"max\": float(np.max(x)),\n",
        "        \"median\": float(q50),  \"q1\": float(q1), \"q5\": float(q5), \"q10\": float(q10),\n",
        "        \"q25\": float(q25), \"q35\": float(q35), \"q65\": float(q65), \"q75\": float(q75),\n",
        "        \"q90\": float(q90), \"q95\": float(q95), \"q99\": float(q99), \"mad\": float(mad), \"iqr\": float(iqr),\n",
        "        \"skew\": float(skew(x, bias=False)) if x.size > 2 else 0.0,\n",
        "        \"kurt\": float(kurtosis(x, fisher=True, bias=False)) if x.size > 3 else 0.0,\n",
        "        \"rms\": float(np.sqrt(np.mean(x**2))),\n",
        "        \"ptp\": float(np.ptp(x)),  # peak-to-peak\n",
        "    }\n",
        "\n",
        "    # additional robust metrics\n",
        "    out.update({\n",
        "        \"cv\": float(out[\"std\"] / (out[\"mean\"] + 1e-12)),   # coefficient of variation\n",
        "        \"range\": float(out[\"max\"] - out[\"min\"]),\n",
        "        \"ratio_max_min\": float(out[\"max\"] / (out[\"min\"] + 1e-12)) if out[\"min\"] != 0 else 0.0,\n",
        "        \"range_iqr_ratio\": float(out[\"ptp\"] / (out[\"iqr\"] + 1e-12)) if out[\"iqr\"] > 0 else 0.0,\n",
        "        \"entropy\": float(-np.sum(p*np.log(p+1e-12)) if (p:=np.histogram(x, bins=\"fd\", density=True)[0]).sum() > 0 else 0.0),\n",
        "    })\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _fft_features(\n",
        "    x: np.ndarray,\n",
        "    n_bands: int = 5,\n",
        "    band_scale: str = \"log\",    # \"equal\" or \"log\"\n",
        "    rolloff_pct: float = 0.85,  # spectral rolloff percentile\n",
        "    use_hann: bool = True,      # taper to reduce leakage\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"spectral features from the (demeaned) 1D signal:\n",
        "      - relative band energies (equal or log-spaced)\n",
        "      - spectral centroid, spread (stdev), rolloff, flatness, flux\n",
        "      - peak frequency (bin index) and peak/median ratio\n",
        "      - spectral entropy\n",
        "    All features are scale-invariant (power-normalized).\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    n = x.size\n",
        "    if n < 8:\n",
        "        # keep schema stable\n",
        "        out.update({f\"fft_band_{i}\": 0.0 for i in range(n_bands)})\n",
        "        out.update({\n",
        "            \"spec_centroid\": 0.0, \"spec_spread\": 0.0, \"spec_rolloff\": 0.0,\n",
        "            \"spec_flatness\": 0.0, \"spec_flux\": 0.0, \"spec_peak_bin\": 0.0,\n",
        "            \"spec_peak_med_ratio\": 0.0, \"spec_entropy\": 0.0\n",
        "        })\n",
        "        return out\n",
        "\n",
        "    # Demean + optional Hann window to reduce leakage\n",
        "    x = x - np.mean(x)\n",
        "    if use_hann:\n",
        "        w = np.hanning(n)\n",
        "        # preserve overall energy scale\n",
        "        x = x * w / (np.sqrt((w**2).mean()) + 1e-12)\n",
        "\n",
        "    # One-sided power spectrum (drop DC)\n",
        "    spec = np.abs(rfft(x))**2\n",
        "    if spec.size <= 1:\n",
        "        spec = np.zeros(2, dtype=float)\n",
        "    spec = spec[1:]  # drop DC\n",
        "    P = spec.astype(float)\n",
        "    P_sum = float(P.sum()) + 1e-12\n",
        "    Pn = P / P_sum              # normalized power (probability over bins)\n",
        "    K = Pn.size\n",
        "\n",
        "    # Frequency bin vector (arbitrary units; bin index is fine for features)\n",
        "    f = np.arange(1, K+1, dtype=float)\n",
        "\n",
        "    # -------- Band energies --------\n",
        "    if n_bands > 0:\n",
        "        if band_scale == \"equal\" or K < n_bands:\n",
        "            edges = np.linspace(0, K, n_bands+1, dtype=int)\n",
        "        else:\n",
        "            # log-spaced edges in bin index space\n",
        "            # (ensure unique, monotone edges within [0, K])\n",
        "            logs = np.linspace(0, 1, n_bands+1)\n",
        "            e = np.unique(np.minimum(K, np.round((K)*(10**logs - 1)/(10 - 1)).astype(int)))\n",
        "            # if uniqueness collapsed (tiny K), fall back to equal\n",
        "            edges = e if e.size == n_bands+1 else np.linspace(0, K, n_bands+1, dtype=int)\n",
        "\n",
        "        band_energies = []\n",
        "        for i in range(len(edges)-1):\n",
        "            a, b = int(edges[i]), int(edges[i+1])\n",
        "            if a >= b:\n",
        "                band_energies.append(0.0)\n",
        "            else:\n",
        "                band_energies.append(float(Pn[a:b].sum()))\n",
        "        # normalize bands to sum to 1 (numerical safety)\n",
        "        be = np.array(band_energies, dtype=float)\n",
        "        be = be / (be.sum() + 1e-12)\n",
        "        out.update({f\"fft_band_{i}\": float(v) for i, v in enumerate(be)})\n",
        "\n",
        "    # -------- Spectral descriptors --------\n",
        "    # centroid & spread (2nd central moment)\n",
        "    centroid = float((f * Pn).sum())\n",
        "    spread = float(np.sqrt(((f - centroid)**2 * Pn).sum()))\n",
        "    out[\"spec_centroid\"] = centroid\n",
        "    out[\"spec_spread\"]   = spread\n",
        "\n",
        "    # rolloff: smallest bin where cumulative power >= rolloff_pct\n",
        "    cdf = np.cumsum(Pn)\n",
        "    roll_idx = int(np.searchsorted(cdf, rolloff_pct, side=\"left\")) + 1  # 1-based-ish\n",
        "    out[\"spec_rolloff\"] = float(roll_idx)\n",
        "\n",
        "    # flatness: geometric mean / arithmetic mean (on power)\n",
        "    gm = float(np.exp(np.mean(np.log(P + 1e-12))))\n",
        "    am = float(np.mean(P))\n",
        "    out[\"spec_flatness\"] = float(gm / (am + 1e-12))\n",
        "\n",
        "    # spectral flux: L2 difference of successive normalized spectra (single frame ≈ gradient)\n",
        "    # Here, approximate flux as variability across bins:\n",
        "    dPn = np.diff(Pn, prepend=Pn[:1])\n",
        "    out[\"spec_flux\"] = float(np.sqrt((dPn**2).sum()))\n",
        "\n",
        "    # peak frequency bin & peak/median power ratio\n",
        "    peak_bin = int(np.argmax(P)) + 1\n",
        "    medP = float(np.median(P) + 1e-12)\n",
        "    out[\"spec_peak_bin\"] = float(peak_bin)\n",
        "    out[\"spec_peak_med_ratio\"] = float(P.max() / medP)\n",
        "\n",
        "    # spectral entropy (Shannon) of normalized power\n",
        "    out[\"spec_entropy\"] = float(-(Pn * np.log(Pn + 1e-12)).sum() / np.log(K + 1e-12))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _wavelet_energies(x, wavelet=\"db4\", level=None):\n",
        "    \"\"\"Wavelet packet-ish: energy per scale from DWT coefficients.\"\"\"\n",
        "    if x.size < 8:\n",
        "        return {}\n",
        "    coeffs = pywt.wavedec(x - np.mean(x), wavelet=wavelet, level=level)\n",
        "    energies = [np.sum(c**2) for c in coeffs]  # [cA_L, cD_L, ..., cD1]\n",
        "    tot = np.sum(energies) + 1e-12\n",
        "    out = {\"wl_cA\": float(energies[0]/tot)}\n",
        "    for i, e in enumerate(energies[1:], start=1):\n",
        "        out[f\"wl_cD_{i}\"] = float(e/tot)\n",
        "    return out\n",
        "\n",
        "def _raw_n_peaks_and_valleys(x: np.ndarray, n=100) -> dict:\n",
        "    \"\"\"\n",
        "    Extract 100 largest peaks and 100 lowest valleys from the raw signal x.\n",
        "    Returns them as fixed-length features (padded with 0 if shorter).\n",
        "    \"\"\"\n",
        "    if x.size == 0:\n",
        "        return {f\"raw_peak_{i}\": 0.0 for i in range(n)} | {f\"raw_valley_{i}\": 0.0 for i in range(n)}\n",
        "\n",
        "    # Sort values\n",
        "    sorted_vals = np.sort(x)\n",
        "\n",
        "    # Lowest 100\n",
        "    valleys = sorted_vals[:n]\n",
        "    # Highest 100\n",
        "    peaks = sorted_vals[-n:][::-1]  # reverse to descending\n",
        "\n",
        "    # Pad if signal shorter than 2n\n",
        "    valleys = np.pad(valleys, (0, max(0, n - valleys.size)), constant_values=0.0)\n",
        "    peaks   = np.pad(peaks,   (0, max(0, n - peaks.size)),   constant_values=0.0)\n",
        "\n",
        "    # Build feature dictionary\n",
        "    out = {f\"raw_peak_{i}\": float(peaks[i]) for i in range(n)}\n",
        "    out.update({f\"raw_valley_{i}\": float(valleys[i]) for i in range(n)})\n",
        "    return out\n",
        "\n",
        "\n",
        "def _wavelet_energies_dwt(x: np.ndarray, wavelet: str = \"db4\", max_level: int | None = None) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Energy per DWT scale (approx + details), normalized to sum=1.\n",
        "    Stable and fast. Returns fixed keys regardless of x length (given wavelet+level policy).\n",
        "    \"\"\"\n",
        "    if pywt is None or x.size < 8:\n",
        "        return {}\n",
        "    x = x.astype(float, copy=False)\n",
        "    x = x - x.mean()\n",
        "\n",
        "    # choose maximal level by signal length if not provided\n",
        "    if max_level is None:\n",
        "        try:\n",
        "            max_level = pywt.dwt_max_level(len(x), pywt.Wavelet(wavelet).dec_len)\n",
        "            max_level = int(max(1, min(max_level, 6)))  # cap – avoid too deep trees\n",
        "        except Exception:\n",
        "            max_level = 4\n",
        "\n",
        "    coeffs = pywt.wavedec(x, wavelet=wavelet, level=max_level)\n",
        "    energies = [float(np.sum(c.astype(float)**2)) for c in coeffs]  # [cA_L, cD_L, ..., cD1]\n",
        "    tot = float(np.sum(energies)) + 1e-12\n",
        "    out = {\"wl_dwt_cA\": float(energies[0] / tot)}\n",
        "    for i, e in enumerate(energies[1:], start=1):\n",
        "        out[f\"wl_dwt_cD_{i}\"] = float(e / tot)\n",
        "\n",
        "    # extra shape descriptors across scales\n",
        "    e_arr = np.array(energies, dtype=float) / tot\n",
        "    idx = np.arange(len(e_arr), dtype=float)\n",
        "    out[\"wl_dwt_scale_centroid\"] = float((idx * e_arr).sum() / (e_arr.sum() + 1e-12))\n",
        "    out[\"wl_dwt_entropy\"] = float(-np.sum(e_arr * np.log(e_arr + 1e-12)) / np.log(len(e_arr) + 1e-12))\n",
        "    return out\n",
        "\n",
        "def _segment_features(x):\n",
        "    \"\"\"Compose robust stats + spectral features for one segment.\"\"\"\n",
        "    f = {}\n",
        "    f.update(_stats_robust(x))\n",
        "    f.update(_fft_features(x, n_bands=5))\n",
        "    f.update(_wavelet_energies(x))\n",
        "    f.update(_raw_n_peaks_and_valleys(x,n=400))\n",
        "    return f\n",
        "\n",
        "def extract_features_rich(\n",
        "    X: pd.DataFrame,\n",
        "    windows: tuple[int, ...] = (500,),   # add more, e.g., (64, 256, 512)\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Per-id features using value and period columns + optional windowed contrasts.\"\"\"\n",
        "    feats = []\n",
        "    print_count = 0\n",
        "\n",
        "    for id_, g in X.groupby(level=\"id\"):\n",
        "        if print_count % 500 == 0 and print_count > 0:\n",
        "            print(f\"...features extracted from samples id: 0-{id_}\")\n",
        "        print_count += 1\n",
        "\n",
        "        # Pull arrays\n",
        "        v = g[\"value\"].to_numpy(dtype=float, copy=False)\n",
        "        p = g[\"period\"].to_numpy(dtype=int, copy=False)\n",
        "\n",
        "        # Boundary: first post-break index; if no post segment, boundary=end\n",
        "        if np.any(p == 1):\n",
        "            boundary_idx = int(np.argmax(p == 1))  # first True\n",
        "        else:\n",
        "            boundary_idx = len(v)\n",
        "\n",
        "        # Full pre/post segments\n",
        "        pre = v[:boundary_idx]\n",
        "        post = v[boundary_idx:]\n",
        "\n",
        "        d = {\"id\": id_}\n",
        "\n",
        "        # --- Global features\n",
        "        d.update({f\"g_{k}\": val for k, val in _segment_features(v).items()})\n",
        "\n",
        "        # --- Pre/Post features\n",
        "        d.update({f\"pre_{k}\": val for k, val in _segment_features(pre).items()})\n",
        "        d.update({f\"post_{k}\": val for k, val in _segment_features(post).items()})\n",
        "\n",
        "        # --- Deltas (post - pre) for key stats\n",
        "        for k in [\"mean\", \"std\", \"median\", \"mad\", \"skew\", \"kurt\", \"rms\"]:\n",
        "            d[f\"delta_{k}\"] = d.get(f\"post_{k}\", 0.0) - d.get(f\"pre_{k}\", 0.0)\n",
        "\n",
        "        # --- Windowed features around boundary (pre/post windows per size)\n",
        "        for W in windows:\n",
        "            # slice bounds\n",
        "            pre_win = v[max(0, boundary_idx - W): boundary_idx]\n",
        "            post_win = v[boundary_idx: min(len(v), boundary_idx + W)]\n",
        "\n",
        "            # stats per window\n",
        "            pre_feats = _segment_features(pre_win)\n",
        "            post_feats = _segment_features(post_win)\n",
        "\n",
        "            # prefix & attach\n",
        "            d.update({f\"pre_win{W}_{k}\": val for k, val in pre_feats.items()})\n",
        "            d.update({f\"post_win{W}_{k}\": val for k, val in post_feats.items()})\n",
        "\n",
        "            # deltas per window\n",
        "            for k in [\"mean\", \"std\", \"median\", \"mad\", \"skew\", \"kurt\", \"rms\"]:\n",
        "                d[f\"delta_win{W}_{k}\"] = post_feats.get(k, 0.0) - pre_feats.get(k, 0.0)\n",
        "\n",
        "            \"\"\"for k in [\"spec_centroid\", \"spec_spread\", \"spec_rolloff\", \"spec_flatness\", \"spec_flux\",\n",
        "                      \"spec_peak_bin\", \"spec_peak_med_ratio\", \"spec_entropy\"]:\n",
        "                          d[f\"delta_{k}\"] = d.get(f\"post_{k}\", 0.0) - d.get(f\"pre_{k}\", 0.0)\n",
        "            \"\"\"\n",
        "            # counts & ratio for this window\n",
        "            len_pre_w = pre_win.size\n",
        "            len_post_w = post_win.size\n",
        "            d[f\"len_pre_win{W}\"] = int(len_pre_w)\n",
        "            d[f\"len_post_win{W}\"] = int(len_post_w)\n",
        "            d[f\"ratio_post_pre_win{W}\"] = float(len_post_w / (len_pre_w + 1e-6))\n",
        "\n",
        "        # --- basic counts\n",
        "        d[\"len_total\"] = int(v.size)\n",
        "        d[\"n_pre\"] = int(pre.size)\n",
        "        d[\"n_post\"] = int(post.size)\n",
        "        d[\"ratio_post_pre\"] = float(d[\"n_post\"] / (d[\"n_pre\"] + 1e-6))\n",
        "\n",
        "        # collect\n",
        "        feats.append(d)\n",
        "\n",
        "    df = pd.DataFrame(feats).set_index(\"id\")\n",
        "    df = df.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.svm import NuSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "try: # Optional: XGBoost (guarded import)\n",
        "    from xgboost import XGBClassifier\n",
        "    _HAS_XGB = True\n",
        "except Exception:\n",
        "    _HAS_XGB = False\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import NuSVC, SVC\n",
        ""
      ],
      "metadata": {
        "id": "AhICW6hCzNlP"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def _safe_cv(n_splits=5, seed=42):\n",
        "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "def _supports_sample_weight(estimator) -> bool:\n",
        "    # crude but practical check\n",
        "    return \"sample_weight\" in estimator.fit.__code__.co_varnames\n",
        "\n",
        "def _ensure_proba_model(model, method=\"sigmoid\", cv=3, n_jobs=1, random_state=42):\n",
        "    \"\"\"\n",
        "    Ensure we can call predict_proba(). If the model lacks it, wrap with CalibratedClassifierCV.\n",
        "    For NuSVC we set probability=True already, but calibration can be more stable on small data.\n",
        "    \"\"\"\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        return model\n",
        "    return CalibratedClassifierCV(model, method=method, cv=cv, n_jobs=n_jobs)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Tuners (cross val + grid search) per model type\n",
        "# ---------------------------\n",
        "\n",
        "def tune_rfc(Xf, y, n_jobs=2, verbose=3, seed=42):\n",
        "    \"\"\" RandomForestClassifier tuner\"\"\"\n",
        "    base = RandomForestClassifier(random_state=seed, n_estimators=300)\n",
        "    grid = {\n",
        "        \"n_estimators\": [200, 300, 500],\n",
        "        \"max_depth\": [None, 8, 12],\n",
        "        \"min_samples_leaf\": [1, 2, 5],\n",
        "        \"max_features\": [\"sqrt\", 0.5, None],\n",
        "    }\n",
        "    cv = _safe_cv(5, seed)\n",
        "    sw = compute_sample_weight(\"balanced\", y.astype(int))\n",
        "    gs = GridSearchCV(\n",
        "        base, grid, scoring=\"roc_auc\", cv=cv,\n",
        "        n_jobs=n_jobs, pre_dispatch=n_jobs, refit=True, verbose=verbose, return_train_score=False\n",
        "    )\n",
        "    if _supports_sample_weight(base):\n",
        "        gs.fit(Xf, y.astype(int), sample_weight=sw)\n",
        "    else:\n",
        "        gs.fit(Xf, y.astype(int))\n",
        "    print(\"RFC BEST AUC:\", gs.best_score_, \"PARAMS:\", gs.best_params_)\n",
        "    return gs.best_estimator_\n",
        "\n",
        "def tune_hgb(Xf, y, n_jobs=2, verbose=3, seed=42):\n",
        "    \"\"\" HistGradientBoostingClassifier tuner\"\"\"\n",
        "    est = HistGradientBoostingClassifier( random_state=seed, early_stopping=True,\n",
        "                                         validation_fraction=0.15, max_iter=1000 )\n",
        "    grid = {\n",
        "        \"learning_rate\":     [0.02, 0.04, 0.06],\n",
        "        \"max_depth\":         [6, 10, 15],\n",
        "        \"min_samples_leaf\":  [20, 50],\n",
        "        \"l2_regularization\": [5e-4, 1e-3, 1e-2],\n",
        "        \"max_bins\":          [63, 127]\n",
        "    }\n",
        "    cv = _safe_cv(5, seed)\n",
        "    sw = compute_sample_weight(\"balanced\", y.astype(int))\n",
        "    gs = GridSearchCV(\n",
        "        est, grid, scoring=\"roc_auc\", cv=cv,\n",
        "        n_jobs=n_jobs, pre_dispatch=n_jobs, refit=True, verbose=verbose, return_train_score=False\n",
        "    )\n",
        "    # HGB supports sample_weight\n",
        "    gs.fit(Xf, y.astype(int), sample_weight=sw)\n",
        "    print(\"HGB BEST AUC:\", gs.best_score_, \"PARAMS:\", gs.best_params_)\n",
        "    return gs.best_estimator_\n",
        "\n",
        "def tune_nusvc(Xf, y, n_jobs=2, verbose=3, seed=42):\n",
        "    \"\"\" NuSVC tuner\"\"\"\n",
        "    est = NuSVC(kernel=\"rbf\", probability=True, random_state=seed)\n",
        "    grid = {\n",
        "        \"nu\": [0.1, 0.2, 0.3, 0.5],\n",
        "        \"kernel\": [\"rbf\", \"sigmoid\"],\n",
        "        \"gamma\": [\"scale\", \"auto\"]\n",
        "    }\n",
        "    cv = _safe_cv(3, seed)\n",
        "    sw = compute_sample_weight(\"balanced\", y.astype(int))\n",
        "    gs = GridSearchCV(\n",
        "        est, grid, scoring=\"roc_auc\", cv=cv,\n",
        "        n_jobs=n_jobs, pre_dispatch=n_jobs, refit=True, verbose=verbose, return_train_score=False\n",
        "    )\n",
        "    # NuSVC does NOT support sample_weight directly\n",
        "    gs.fit(Xf, y.astype(int))\n",
        "    print(\"NuSVC BEST AUC:\", gs.best_score_, \"PARAMS:\", gs.best_params_)\n",
        "    return gs.best_estimator_\n",
        "\n",
        "\n",
        "def tune_xgb(Xf, y, n_jobs=2, verbose=3, seed=42):\n",
        "    \"\"\" XGBClassifier tuner\"\"\"\n",
        "    if not _HAS_XGB:\n",
        "        raise RuntimeError(\"XGBoost not installed/available\")\n",
        "\n",
        "    est = XGBClassifier(random_state=seed, eval_metric=\"logloss\",\n",
        "                        tree_method=\"hist\",  n_estimators=500)\n",
        "    grid = {\n",
        "        \"max_depth\": [5, 10, 15],\n",
        "        \"learning_rate\": [0.05, 0.1],\n",
        "        \"subsample\": [0.7, 1.0],\n",
        "        \"colsample_bytree\": [0.7, 1.0],\n",
        "        \"min_child_weight\": [1, 5]\n",
        "    }\n",
        "    cv = _safe_cv(5, seed)\n",
        "    sw = compute_sample_weight(\"balanced\", y.astype(int))\n",
        "    gs = GridSearchCV(\n",
        "        est, grid, scoring=\"roc_auc\", cv=cv,\n",
        "        n_jobs=n_jobs, pre_dispatch=n_jobs, refit=True, verbose=verbose, return_train_score=False\n",
        "    )\n",
        "    gs.fit(Xf, y.astype(int), sample_weight=sw)\n",
        "    print(\"XGB BEST AUC:\", gs.best_score_, \"PARAMS:\", gs.best_params_)\n",
        "    return gs.best_estimator_\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Model registry & builder\n",
        "# ---------------------------\n",
        "def get_model(model_class: str, seed: int = 42):\n",
        "\n",
        "    model_class = model_class.strip()\n",
        "\n",
        "    if model_class == \"RandomForestClassifier\":\n",
        "        return RandomForestClassifier(n_estimators=300, random_state=seed)\n",
        "\n",
        "    elif model_class == \"HistGradientBoostingClassifier\":\n",
        "        return HistGradientBoostingClassifier(\n",
        "            random_state=seed, early_stopping=True, validation_fraction=0.10,\n",
        "            max_iter=2000, learning_rate=0.04,\n",
        "            max_depth=10, min_samples_leaf=40, l2_regularization=1e-3\n",
        "        )\n",
        "\n",
        "    elif model_class == \"NuSVC\":\n",
        "        return NuSVC(nu=0.5, kernel=\"rbf\", probability=True, random_state=seed)\n",
        "\n",
        "    elif model_class == \"LogisticRegression\":\n",
        "        # always scale for LR\n",
        "        return Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "                         (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=seed))])\n",
        "\n",
        "    elif model_class == \"XGBClassifier\":\n",
        "        if not _HAS_XGB:\n",
        "            raise RuntimeError(\"XGBClassifier requested but xgboost is not available.\")\n",
        "        return XGBClassifier(\n",
        "            random_state=seed, eval_metric=\"logloss\", tree_method=\"hist\",\n",
        "            n_estimators=600, max_depth=5, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_class={model_class}\")\n",
        "\n",
        "\n",
        "def tune_model(model_class: str,\n",
        "               Xf: pd.DataFrame,\n",
        "               y: pd.Series,\n",
        "               n_jobs: int = 2,\n",
        "               seed: int = 42):\n",
        "\n",
        "    if model_class == \"RandomForestClassifier\":\n",
        "        return tune_rfc(Xf, y, n_jobs=n_jobs, seed=seed)\n",
        "\n",
        "    elif model_class == \"HistGradientBoostingClassifier\":\n",
        "        return tune_hgb(Xf, y, n_jobs=n_jobs, seed=seed)\n",
        "\n",
        "    elif model_class == \"NuSVC\":\n",
        "        return tune_nusvc(Xf, y, n_jobs=n_jobs, seed=seed)\n",
        "\n",
        "    elif model_class == \"XGBClassifier\":\n",
        "        return tune_xgb(Xf, y, n_jobs=n_jobs, seed=seed)\n",
        "\n",
        "    elif model_class == \"LogisticRegression\":\n",
        "        # typical LR tuning: C and penalty (kept small here)\n",
        "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))])\n",
        "        grid = {\"clf__C\": [0.1, 1.0, 3.0], \"clf__penalty\": [\"l2\"]}\n",
        "        cv = _safe_cv(5, seed)\n",
        "        gs = GridSearchCV(pipe, grid, scoring=\"roc_auc\", cv=cv, n_jobs=n_jobs, pre_dispatch=n_jobs, refit=True, verbose=2)\n",
        "        gs.fit(Xf, y.astype(int))\n",
        "        print(\"LR BEST AUC:\", gs.best_score_, \"PARAMS:\", gs.best_params_)\n",
        "        return gs.best_estimator_\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"No tuner implemented for model_class={model_class}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pEo9QUj2g6AL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNund6MEEQIH"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "tU33EYoUFCnW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "    is_debug= True,\n",
        "    do_cross_val = False,\n",
        "    model_class: str = \"RandomForestClassifier\",\n",
        "    n_jobs: int = 2,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train on per-id features, align y to ids, save bundle with feature schema.\n",
        "    X_train: MultiIndex (id, time), columns [\"value\",\"period\"]\n",
        "    y_train: Series/DataFrame indexed by id with {0,1} or {False,True}\n",
        "    \"\"\"\n",
        "    os.makedirs(model_directory_path, exist_ok=True)\n",
        "\n",
        "    print(\"TRAINING MODEL CLASS {}\".format(model_class))\n",
        "\n",
        "    # 1) y: squeeze to Series[int], index = ids\n",
        "    if isinstance(y_train, pd.DataFrame):\n",
        "        y_train = y_train.squeeze()\n",
        "    y = y_train.astype(int).copy()\n",
        "\n",
        "    # 2) X: aggregate to per-id features\n",
        "    print(\"Features extraction...\")\n",
        "    if is_debug:\n",
        "          ids_subset = X_train.index.get_level_values(\"id\").unique()[:501]\n",
        "          y = y.loc[y.index.isin(ids_subset)]\n",
        "          X_debug = X_train.loc[X_train.index.get_level_values(\"id\").isin(ids_subset)]\n",
        "          Xf = extract_features_rich(X_debug).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    else:\n",
        "          Xf = extract_features_rich(X_train).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "\n",
        "    # 3) align y to feature rows (ids)\n",
        "    try: # ensure both indices are comparable types\n",
        "        Xf.index = Xf.index.astype(int)\n",
        "        y.index = y.index.astype(int)\n",
        "    except Exception:\n",
        "        pass\n",
        "    y = y.reindex(Xf.index)\n",
        "    if y.isna().any(): # Drop any ids without a label (shouldn't happen on official train)\n",
        "        mask = ~y.isna()\n",
        "        Xf, y = Xf.loc[mask],  y.loc[mask]\n",
        "\n",
        "\n",
        "    # 4) fit model\n",
        "    print(\"Fitting...\")\n",
        "    if do_cross_val: # gird search + cross validation\n",
        "        model = tune_model(model_class, Xf, y, n_jobs=n_jobs, seed=seed)\n",
        "\n",
        "    else: # fit the model with default params\n",
        "        model = get_model(model_class, seed=seed)\n",
        "        sw = compute_sample_weight(\"balanced\", y) if _supports_sample_weight(model) else None\n",
        "        if sw is not None:\n",
        "            model.fit(Xf, y, sample_weight=sw)\n",
        "        else:\n",
        "            model.fit(Xf, y)\n",
        "    print(\"Fitting...done\")\n",
        "\n",
        "    # 5) persist bundle with schema for inference\n",
        "    bundle = {\"model\": model, \"feature_names\": list(Xf.columns), \"model_class\": model_class}\n",
        "    joblib.dump(bundle, os.path.join(model_directory_path, \"model.joblib\"))\n",
        "    print(f\"[train] saved -> {os.path.join(model_directory_path, 'model.joblib')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m6MqPnNFG8s"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "jXwFMASGFCsm"
      },
      "outputs": [],
      "source": [
        "import os, typing as t\n",
        "\n",
        "def _to_feature_row(df_one_id: pd.DataFrame,\n",
        "                    feature_names: t.List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Turn a single-id time series DataFrame into a 1xD feature row,\n",
        "    aligned to 'feature_names' (missing features -> 0.0).\n",
        "    \"\"\"\n",
        "    # If df has MultiIndex (id, time), drop the id level\n",
        "    if isinstance(df_one_id.index, pd.MultiIndex) and \"id\" in df_one_id.index.names:\n",
        "        # Expect a single id per dataset; drop it\n",
        "        df_one_id = df_one_id.droplevel(\"id\")\n",
        "\n",
        "    # Build features for a single id by reusing the rich extractor over a tiny fake batch\n",
        "    # (wrap in a MultiIndex with id=0 temporarily)\n",
        "    tmp = df_one_id.copy()\n",
        "    tmp.index = pd.MultiIndex.from_product([[0], tmp.index], names=[\"id\", \"time\"])\n",
        "    feats = extract_features_rich(tmp)          # returns DataFrame indexed by id\n",
        "    row = feats.iloc[[0]]                       # 1xD\n",
        "    if feature_names:                           # align to saved schema\n",
        "        row = row.reindex(columns=feature_names, fill_value=0.0)\n",
        "    row = row.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    return row\n",
        "\n",
        "def infer(\n",
        "    X_test: t.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    \"\"\"  Load trained model and yield P(y=1) for each incoming dataset. \"\"\"\n",
        "    bundle = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n",
        "\n",
        "    # Support both: (a) raw estimator, (b) dict/bundle with feature_names.\n",
        "    if isinstance(bundle, dict) and \"model\" in bundle:\n",
        "        model = bundle[\"model\"]\n",
        "        feature_names = bundle.get(\"feature_names\", [])\n",
        "    else:\n",
        "        model = bundle\n",
        "        # Try to load feature schema if saved separately\n",
        "        feat_path = os.path.join(model_directory_path, \"feature_names.joblib\")\n",
        "        feature_names = joblib.load(feat_path) if os.path.exists(feat_path) else []\n",
        "\n",
        "    # print(\"\\033[94mYield predictions for model: {}\\033[0m\".format(bundle[\"model_class\"]))\n",
        "\n",
        "    # Handshake: ready\n",
        "    yield\n",
        "\n",
        "    # Iterate ONCE over datasets\n",
        "    for dataset in X_test:\n",
        "        x_row = _to_feature_row(dataset, feature_names)  # Compute 1xD feature row aligned to training schema\n",
        "\n",
        "        # Predict probability for the positive class (y=1: structural break)\n",
        "        proba_pos = float(model.predict_proba(x_row)[:, 1][0])  # scikit-learn's predict_proba returns [:, 1] for the positive class\n",
        "\n",
        "        # Yield a scalar in [0,1]\n",
        "        yield proba_pos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MvPxMaGF9Pc"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Random Forest\n",
        "train(X_train, y_train, model_directory_path='resources', do_cross_val=False, model_class=\"RandomForestClassifier\")\n",
        "crunch.test(force_first_train=False)\n",
        "ROC_AUC_Forest = sklearn.metrics.roc_auc_score( target,    pd.read_parquet(\"data/prediction.parquet\") ) # Call the scoring function\n",
        "print(\"\\033[94m ROC_AUC_Forest = {}\\033[0m\".format(ROC_AUC_Forest))\n",
        "\n",
        "# NuSVC\n",
        "train(X_train, y_train, \"resources\", is_debug= True, do_cross_val=False, model_class=\"NuSVC\")\n",
        "crunch.test(force_first_train=False)\n",
        "ROC_AUC_NuSVC = sklearn.metrics.roc_auc_score( target,    pd.read_parquet(\"data/prediction.parquet\") ) # Call the scoring function\n",
        "print(\"\\033[94m ROC_AUC_NuSVC = {}\\033[0m\".format(ROC_AUC_NuSVC))\n",
        "\n",
        "# LightGBM (if available)\n",
        "train(X_train, y_train, \"resources\", is_debug= True, do_cross_val=False, model_class=\"LogisticRegression\")\n",
        "crunch.test(force_first_train=False)\n",
        "ROC_AUC_LightGBM = sklearn.metrics.roc_auc_score( target,    pd.read_parquet(\"data/prediction.parquet\") ) # Call the scoring funct\n",
        "print(\"\\033[94m ROC_AUC = {}\\033[0m\".format(ROC_AUC_LightGBM))\n",
        "\n",
        "# XGBoost (if available)\n",
        "train(X_train, y_train, \"resources\", is_debug= True, do_cross_val=False, model_class=\"XGBClassifier\")\n",
        "crunch.test(force_first_train=False)\n",
        "ROC_AUC_XGBoost = sklearn.metrics.roc_auc_score( target,    pd.read_parquet(\"data/prediction.parquet\") ) # Call the scoring funct\n",
        "print(\"\\033[94m ROC_AUC = {}\\033[0m\".format(ROC_AUC_XGBoost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkCwp4oi6TWR",
        "outputId": "c90c8905-5647-465f-83fe-5f74d1bed3e9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODEL CLASS RandomForestClassifier\n",
            "FEATURES EXTRACTION...\n",
            "Progress report...extracting features from id: 500\n",
            "Fitting...\n",
            "Fitting...done\n",
            "[train] saved -> resources/model.joblib\n",
            "ignoring cell #5: invalid syntax (<unknown>, line 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:49:29 no forbidden library found\n",
            "08:49:29 \n",
            "08:49:30 started\n",
            "08:49:30 running local test\n",
            "08:49:30 internet access isn't restricted, no check will be done\n",
            "08:49:30 \n",
            "08:49:31 starting unstructured loop...\n",
            "08:49:31 executing - command=infer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:49:43 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
            "08:49:43 executing - command=infer\n",
            "08:49:46 determinism check: passed\n",
            "08:49:46 save prediction - path=data/prediction.parquet\n",
            "08:49:46 ended\n",
            "08:49:46 duration - time=00:00:16\n",
            "08:49:46 memory - before=\"1.56 GB\" after=\"1.55 GB\" consumed=\"-3145728 bytes\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m ROC_AUC_Forest = 0.5401408450704226\u001b[0m\n",
            "TRAINING MODEL CLASS NuSVC\n",
            "FEATURES EXTRACTION...\n",
            "Progress report...extracting features from id: 500\n",
            "Fitting...\n",
            "Fitting...done\n",
            "[train] saved -> resources/model.joblib\n",
            "ignoring cell #5: invalid syntax (<unknown>, line 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:50:01 no forbidden library found\n",
            "08:50:01 \n",
            "08:50:01 started\n",
            "08:50:01 running local test\n",
            "08:50:01 internet access isn't restricted, no check will be done\n",
            "08:50:01 \n",
            "08:50:02 starting unstructured loop...\n",
            "08:50:02 executing - command=infer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:50:12 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
            "08:50:12 executing - command=infer\n",
            "08:50:16 determinism check: passed\n",
            "08:50:16 save prediction - path=data/prediction.parquet\n",
            "08:50:16 ended\n",
            "08:50:16 duration - time=00:00:14\n",
            "08:50:16 memory - before=\"1.55 GB\" after=\"1.54 GB\" consumed=\"-17809408 bytes\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m ROC_AUC_NuSVC = 0.49436619718309865\u001b[0m\n",
            "TRAINING MODEL CLASS LogisticRegression\n",
            "FEATURES EXTRACTION...\n",
            "Progress report...extracting features from id: 500\n",
            "Fitting...\n",
            "Fitting...done\n",
            "[train] saved -> resources/model.joblib\n",
            "ignoring cell #5: invalid syntax (<unknown>, line 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:50:35 no forbidden library found\n",
            "08:50:35 \n",
            "08:50:35 started\n",
            "08:50:35 running local test\n",
            "08:50:35 internet access isn't restricted, no check will be done\n",
            "08:50:35 \n",
            "08:50:36 starting unstructured loop...\n",
            "08:50:36 executing - command=infer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:50:47 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
            "08:50:47 executing - command=infer\n",
            "08:50:49 determinism check: passed\n",
            "08:50:50 save prediction - path=data/prediction.parquet\n",
            "08:50:50 ended\n",
            "08:50:50 duration - time=00:00:14\n",
            "08:50:50 memory - before=\"1.55 GB\" after=\"1.55 GB\" consumed=\"-1392640 bytes\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m ROC_AUC = 0.3704225352112676\u001b[0m\n",
            "TRAINING MODEL CLASS XGBClassifier\n",
            "FEATURES EXTRACTION...\n",
            "Progress report...extracting features from id: 500\n",
            "Fitting...\n",
            "Fitting...done\n",
            "[train] saved -> resources/model.joblib\n",
            "ignoring cell #5: invalid syntax (<unknown>, line 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:54:05 no forbidden library found\n",
            "08:54:05 \n",
            "08:54:05 started\n",
            "08:54:05 running local test\n",
            "08:54:05 internet access isn't restricted, no check will be done\n",
            "08:54:05 \n",
            "08:54:06 starting unstructured loop...\n",
            "08:54:06 executing - command=infer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:54:47 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
            "08:54:47 executing - command=infer\n",
            "08:55:00 determinism check: passed\n",
            "08:55:00 save prediction - path=data/prediction.parquet\n",
            "08:55:00 ended\n",
            "08:55:00 duration - time=00:00:54\n",
            "08:55:00 memory - before=\"1.58 GB\" after=\"1.58 GB\" consumed=\"0 bytes\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m ROC_AUC = 0.4863849765258216\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvJzmEntF6ZV",
        "outputId": "651c3718-6eb2-444e-897a-3826501f65c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ignoring cell #5: invalid syntax (<unknown>, line 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:27:46 no forbidden library found\n",
            "08:27:46 \n",
            "08:27:47 started\n",
            "08:27:47 running local test\n",
            "08:27:47 internet access isn't restricted, no check will be done\n",
            "08:27:47 \n",
            "08:27:48 starting unstructured loop...\n",
            "08:27:48 executing - command=train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n",
            "Progress report...extracting features per id\n",
            "Progress report...extracting features from id: 500\n",
            "Progress report...fitting model RandomForestClassifier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:28:13 executing - command=infer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress report...fitting the model....done\n",
            "[train] saved -> resources/model.joblib\n",
            "RandomForestClassifier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:28:26 checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
            "08:28:26 executing - command=infer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "08:28:30 determinism check: passed\n",
            "08:28:30 save prediction - path=data/prediction.parquet\n",
            "08:28:30 ended\n",
            "08:28:30 duration - time=00:00:43\n",
            "08:28:30 memory - before=\"1.43 GB\" after=\"1.47 GB\" consumed=\"38.65 MB\"\n"
          ]
        }
      ],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "eYePTwGyF6Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8603a46c-a2c9-4184-f8ea-930691e0e8c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.5401408450704226)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# Load the predictions and the targets\n",
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "sklearn.metrics.roc_auc_score( target,    prediction) # Call the scoring function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1LPCHmVRvL_"
      },
      "source": [
        " - Best ROC_AUC score so far -> 0.7427230  (HistGradientBoostingClassifier)\n",
        " - ....(addign CWT scalogram features (Complex Shannon via spkit))\n",
        " - ....("
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeomHep3DZ7wMnbjrus2lp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}