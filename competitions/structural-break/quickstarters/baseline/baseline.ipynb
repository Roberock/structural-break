{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWIItAe-0fN"
      },
      "source": [
        "[![Open In Colab](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/badge/open-in-colab.svg)](https://colab.research.google.com/github/crunchdao/quickstarters/blob/master/competitions/structural-break/quickstarters/baseline/baseline.ipynb)\n",
        "[![Open In Kaggle](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/badge/open-in-kaggle.svg)](https://www.kaggle.com/code/crunchdao/structural-break-baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurIF1Ve-0fP"
      },
      "source": [
        "# ADIA Lab Structural Break Challenge\n",
        "\n",
        "## Challenge Overview\n",
        "\n",
        "Welcome to the ADIA Lab Structural Break Challenge! In this challenge, you will analyze univariate time series data to determine whether a structural break has occurred at a specified boundary point.\n",
        "\n",
        "### What is a Structural Break?\n",
        "\n",
        "A structural break occurs when the process governing the data generation changes at a certain point in time. These changes can be subtle or dramatic, and detecting them accurately is crucial across various domains such as climatology, industrial monitoring, finance, and healthcare.\n",
        "\n",
        "\n",
        "### Your Task\n",
        "\n",
        "For each time series in the test set, you need to predict a score between `0` and `1`:\n",
        "- Values closer to `0` indicate no structural break at the specified boundary point;\n",
        "- Values closer to `1` indicate a structural break did occur.\n",
        "\n",
        "### Evaluation Metric\n",
        "\n",
        "The evaluation metric is [ROC AUC (Area Under the Receiver Operating Characteristic Curve)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), which measures the performance of detection algorithms regardless of their specific calibration.\n",
        "\n",
        "- ROC AUC around `0.5`: No better than random chance;\n",
        "- ROC AUC approaching `1.0`: Perfect detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbJYIv6e0sGQ"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The first steps to get started are:\n",
        "1. Get the setup command\n",
        "2. Execute it in the cell below\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Reveal token](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/reveal-token.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DUeixiC_IJM",
        "outputId": "7dbffb2d-9b28-4f77-8d63-bac4059c1665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crunch-cli, version 7.4.0\n",
            "delete /content/.crunchdao\n",
            "you appear to have never submitted code before\n",
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "                                \n",
            "---\n",
            "Success! Your environment has been correctly setup.\n",
            "Next recommended actions:\n",
            "1. Load the Crunch Toolings: `crunch = crunch.load_notebook()`\n",
            "2. Execute the cells with your code\n",
            "3. Run a test: `crunch.test()`\n",
            "4. Download and submit your code to the platform!\n"
          ]
        }
      ],
      "source": [
        "# Install the Crunch CLI\n",
        "%pip install crunch-cli --upgrade --quiet --progress-bar off\n",
        "\n",
        "# Setup your local environment\n",
        "!crunch setup-notebook structural-break sSwhgkE7cS2nPb7aAet6thaR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IBhw7hv-0fQ"
      },
      "source": [
        "# Your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpLeMWSw-0fQ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "# Import your dependencies\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjD_WSAS-0fR",
        "outputId": "d4fb07f1-1c6d-40cb-c26b-72b2bf499388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded inline runner with module: <module '__main__'>\n",
            "\n",
            "cli version: 7.4.0\n",
            "available ram: 12.67 gb\n",
            "available cpu: 2 core\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "import crunch\n",
        "\n",
        "# Load the Crunch Toolings\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiKJODFx-0fR"
      },
      "source": [
        "## Understanding the Data\n",
        "\n",
        "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
        "\n",
        "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKHXgvjN-0fS",
        "outputId": "1886ff22-f58f-464d-a5f0-a5f385c660b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
            "data/X_train.parquet: already exists, file length match\n",
            "data/X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
            "data/X_test.reduced.parquet: already exists, file length match\n",
            "data/y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
            "data/y_train.parquet: already exists, file length match\n",
            "data/y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
            "data/y_test.reduced.parquet: already exists, file length match\n"
          ]
        }
      ],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.stats import kurtosis, skew\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "import joblib\n",
        "\n",
        "# --- Rich features: robust stats + higher moments + FFT + wavelets ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "from numpy.fft import rfft\n",
        "import pywt  # pip install PyWavelets"
      ],
      "metadata": {
        "id": "Ux6-lSQ01UVN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _stats_robust(x):\n",
        "    if x.size == 0:\n",
        "        return {}\n",
        "    q5, q10, q25, q50, q75, q90, q95 = np.percentile(x, [5, 10, 25, 50, 75, 90, 95])\n",
        "    mad = np.median(np.abs(x - q50))  # Median Absolute Deviation\n",
        "    return {\n",
        "        \"mean\": np.mean(x), \"std\": np.std(x),\n",
        "        \"min\": np.min(x),  \"max\": np.max(x),  \"median\": q50,\n",
        "        \"q5\": q5, \"q10\": q10, \"q25\": q25, \"q75\": q75, \"q90\": q90, \"q95\": q95,\n",
        "        \"mad\": mad, \"skew\": skew(x, bias=False) if x.size > 2 else 0.0, \"kurt\": kurtosis(x, fisher=True, bias=False) if x.size > 3 else 0.0,\n",
        "        \"rms\": np.sqrt(np.mean(x**2)),  \"ptp\": np.ptp(x),  # peak-to-peak\n",
        "    }\n",
        "\n",
        "def _fft_energy_bands(x, n_bands=5):\n",
        "    \"\"\"Simple relative-band energies from power spectrum.\"\"\"\n",
        "    if x.size < 8:\n",
        "        return {f\"fft_band_{i}\": 0.0 for i in range(n_bands)}\n",
        "    spec = np.abs(rfft(x - np.mean(x)))**2\n",
        "    spec = spec[1:]  # drop DC for energy ratios\n",
        "    if spec.size == 0:\n",
        "        return {f\"fft_band_{i}\": 0.0 for i in range(n_bands)}\n",
        "    # split into equal bands\n",
        "    bands = np.array_split(spec, n_bands)\n",
        "    energies = np.array([b.sum() for b in bands], dtype=float)\n",
        "    tot = energies.sum() + 1e-12\n",
        "    return {f\"fft_band_{i}\": float(e/tot) for i, e in enumerate(energies)}\n",
        "\n",
        "def _wavelet_energies(x, wavelet=\"db4\", level=None):\n",
        "    \"\"\"Wavelet packet-ish: energy per scale from DWT coefficients.\"\"\"\n",
        "    if x.size < 8:\n",
        "        return {}\n",
        "    coeffs = pywt.wavedec(x - np.mean(x), wavelet=wavelet, level=level)\n",
        "    energies = [np.sum(c**2) for c in coeffs]  # [cA_L, cD_L, ..., cD1]\n",
        "    tot = np.sum(energies) + 1e-12\n",
        "    out = {\"wl_cA\": float(energies[0]/tot)}\n",
        "    for i, e in enumerate(energies[1:], start=1):\n",
        "        out[f\"wl_cD_{i}\"] = float(e/tot)\n",
        "    return out\n",
        "\n",
        "def _segment_features(x):\n",
        "    \"\"\"Compose robust stats + spectral features for one segment.\"\"\"\n",
        "    f = {}\n",
        "    f.update(_stats_robust(x))\n",
        "    f.update(_fft_energy_bands(x, n_bands=5))\n",
        "    f.update(_wavelet_energies(x))\n",
        "    return f\n",
        "\n",
        "def extract_features_rich(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Per-id features using value and period columns.\"\"\"\n",
        "    feats = []\n",
        "    print_count = 0\n",
        "    for id_, g in X.groupby(level=\"id\"):\n",
        "\n",
        "        if print_count == 1_000:\n",
        "            print('Progress report...extracting features from id: {}'.format(id_))\n",
        "            print_count = 0\n",
        "        print_count += 1\n",
        "\n",
        "        v = g[\"value\"].values.astype(float)\n",
        "        pre = g.loc[g[\"period\"] == 0, \"value\"].values.astype(float)\n",
        "        post = g.loc[g[\"period\"] == 1, \"value\"].values.astype(float)\n",
        "\n",
        "        d = {\"id\": id_}\n",
        "        # global\n",
        "        d.update({f\"g_{k}\": v for k, v in _segment_features(v).items()})\n",
        "\n",
        "        # pre/post\n",
        "        d.update({f\"pre_{k}\": v for k, v in _segment_features(pre).items()})\n",
        "        d.update({f\"post_{k}\": v for k, v in _segment_features(post).items()})\n",
        "\n",
        "        # deltas (post - pre) for key stats\n",
        "        for k in [\"mean\", \"std\", \"median\", \"mad\", \"skew\", \"kurt\", \"rms\"]:\n",
        "            d[f\"delta_{k}\"] = (d.get(f\"post_{k}\", 0.0) - d.get(f\"pre_{k}\", 0.0))\n",
        "\n",
        "        # counts & ratio\n",
        "        d[\"len_total\"] = int(v.size)\n",
        "        d[\"n_pre\"] = int(pre.size)\n",
        "        d[\"n_post\"] = int(post.size)\n",
        "        d[\"ratio_post_pre\"] = float(d[\"n_post\"]/(d[\"n_pre\"]+1e-6))\n",
        "        feats.append(d)\n",
        "\n",
        "    df = pd.DataFrame(feats).set_index(\"id\")\n",
        "    return df.replace([np.inf, -np.inf], 0.0).fillna(0.0)"
      ],
      "metadata": {
        "id": "s0-o_lsC1ZqV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "\n",
        "    X_train_moments = extract_features_rich(X_train).fillna(0.0)\n",
        "    model = HistGradientBoostingClassifier(random_state=42, l2_regularization=l2_regularization, max_leaf_nodes=max_leaf_nodes)\n",
        "    model.fit(X_train, y_train)\n",
        "    model = None\n",
        "    # You could enhance this by training an actual model, for example:\n",
        "    # 1. Extract features from before/after segments of each time series\n",
        "    # 2. Train a classifier using these features and y_train labels\n",
        "    # 3. Save the trained model\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        # Baseline approach: Compute t-test between values before and after boundary point\n",
        "        # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "        # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "        # suggesting a structural break\n",
        "        def t_test(u: pd.DataFrame):\n",
        "            return -scipy.stats.ttest_ind(\n",
        "                u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "                u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "            ).pvalue\n",
        "\n",
        "        prediction = t_test(dataset)\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "        # Note: This baseline approach uses a t-test to compare the distributions\n",
        "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "        # suggests stronger evidence that the distributions are different,\n",
        "        # indicating a potential structural break."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZeP-4--0fU"
      },
      "outputs": [],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AE1i3pR-0fV"
      },
      "source": [
        "# Submit your Notebook\n",
        "\n",
        "To submit your work, you must:\n",
        "1. Download your Notebook from Colab\n",
        "2. Upload it to the platform\n",
        "3. Create a run to validate it\n",
        "\n",
        "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
        "\n",
        "![Download and Submit Notebook](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/download-and-submit-notebook.gif)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}