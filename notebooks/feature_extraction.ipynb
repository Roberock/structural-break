{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "\n",
    "RESOURCES_DIR = Path(\"../resources\")\n",
    "RESOURCES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# FAST FEATURE EXTRACTORS\n",
    "# -------------------------------\n",
    "def _agg_basic_stats(df, value_col):\n",
    "    agg = {\n",
    "        value_col: [\n",
    "            \"mean\", \"std\", \"min\", \"max\", \"median\",\n",
    "            lambda s: s.quantile(0.10),\n",
    "            lambda s: s.quantile(0.90),\n",
    "            \"skew\", \"kurt\"\n",
    "        ]\n",
    "    }\n",
    "    g = df.groupby(level=\"id\")[value_col].agg(agg[value_col])\n",
    "    g.columns = [\n",
    "        \"mean\",\"std\",\"min\",\"max\",\"median\",\"q10\",\"q90\",\"skew\",\"kurt\"\n",
    "    ]\n",
    "    return g.add_prefix(\"whole_\")\n",
    "\n",
    "def _agg_segment_stats(df, value_col, period_col, seg):\n",
    "    \"\"\"seg=0 or 1 (pre/post).\"\"\"\n",
    "    sub = df[df[period_col] == seg]\n",
    "    if sub.empty:\n",
    "        # no rows for this segment\n",
    "        return pd.DataFrame(index=df.index.get_level_values(\"id\").unique()).assign(\n",
    "            **{f\"{'pre' if seg==0 else 'post'}_{n}\": 0.0 for n in\n",
    "               [\"mean\",\"std\",\"min\",\"max\",\"median\",\"q10\",\"q90\",\"skew\",\"kurt\"]}\n",
    "        )\n",
    "    agg = sub.groupby(level=\"id\")[value_col].agg([\n",
    "        \"mean\",\"std\",\"min\",\"max\",\"median\",\n",
    "        lambda s: s.quantile(0.10),\n",
    "        lambda s: s.quantile(0.90),\n",
    "        \"skew\",\"kurt\"\n",
    "    ])\n",
    "    agg.columns = [\"mean\",\"std\",\"min\",\"max\",\"median\",\"q10\",\"q90\",\"skew\",\"kurt\"]\n",
    "    return agg.add_prefix(\"pre_\" if seg == 0 else \"post_\")\n",
    "\n",
    "def _length_feats(df, period_col):\n",
    "    g = df.groupby(level=\"id\")[period_col]\n",
    "    n_total = g.size().rename(\"n_total\")\n",
    "    # counts by value\n",
    "    counts = g.value_counts().unstack(fill_value=0)\n",
    "    counts = counts.rename(columns={0: \"n_pre\", 1: \"n_post\"})\n",
    "    if \"n_pre\" not in counts: counts[\"n_pre\"] = 0\n",
    "    if \"n_post\" not in counts: counts[\"n_post\"] = 0\n",
    "    out = pd.concat([n_total, counts[[\"n_pre\",\"n_post\"]]], axis=1).fillna(0)\n",
    "    out[\"post_pre_ratio\"] = np.where(out[\"n_pre\"] > 0, out[\"n_post\"] / out[\"n_pre\"], 0.0)\n",
    "    return out\n",
    "\n",
    "def _ewm_tail_last(df, value_col, alphas=(0.1, 0.01, 0.001)):\n",
    "    \"\"\"Exponentially-weighted stats per id (last value only). Vectorised via groupby.transform + last().\"\"\"\n",
    "    out = pd.DataFrame(index=df.index.get_level_values(\"id\").unique())\n",
    "    # we need the last index per id to pick tail values\n",
    "    last_idx = df.groupby(level=\"id\").tail(1).index\n",
    "    for a in alphas:\n",
    "        mean_series = df.groupby(level=\"id\")[value_col].transform(lambda s: s.ewm(alpha=a, adjust=False).mean())\n",
    "        var_series  = df.groupby(level=\"id\")[value_col].transform(lambda s: s.ewm(alpha=a, adjust=False).var(bias=False))\n",
    "        mean_last = mean_series.loc[last_idx]\n",
    "        std_last  = var_series.loc[last_idx].clip(lower=0).pow(0.5)\n",
    "        mean_last.index = mean_last.index.droplevel(\"time\")\n",
    "        std_last.index  = std_last.index.droplevel(\"time\")\n",
    "        out[f\"ewm{a:g}_mean_last\"] = mean_last.astype(float)\n",
    "        out[f\"ewm{a:g}_std_last\"]  = std_last.astype(float).fillna(0.0)\n",
    "    return out\n",
    "\n",
    "def _fft_band_powers_fast(df, value_col=\"value\", Nf=12):\n",
    "    \"\"\"\n",
    "    Fast-ish rFFT per id by operating on contiguous blocks.\n",
    "    Assumes df is sorted by (id, time). Uses NumPy to minimise pandas overhead.\n",
    "    \"\"\"\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "\n",
    "    ids = df.index.get_level_values(\"id\").to_numpy()\n",
    "    vals = df[value_col].to_numpy(dtype=np.float64)\n",
    "\n",
    "    # boundaries where id changes\n",
    "    boundaries = np.concatenate([[0], np.flatnonzero(ids[1:] != ids[:-1]) + 1, [len(ids)]])\n",
    "    out_rows = {}\n",
    "    for b0, b1 in zip(boundaries[:-1], boundaries[1:]):\n",
    "        gid = int(ids[b0])\n",
    "        v = vals[b0:b1]\n",
    "        if v.size < 2:\n",
    "            out_rows[gid] = {f\"fpow_{k}\": 0.0 for k in range(1, Nf+1)}\n",
    "            continue\n",
    "        v = v - v.mean()\n",
    "        power = np.abs(np.fft.rfft(v))**2\n",
    "        band = power[1:1+Nf]\n",
    "        if band.size < Nf:\n",
    "            band = np.pad(band, (0, Nf - band.size))\n",
    "        total = power.sum()\n",
    "        norm = (band / total) if total > 0 else np.zeros_like(band)\n",
    "        out_rows[gid] = {f\"fpow_{k}\": float(norm[k-1]) for k in range(1, Nf+1)}\n",
    "    return pd.DataFrame.from_dict(out_rows, orient=\"index\").rename_axis(\"id\").fillna(0.0)\n",
    "\n",
    "def f_x2z_expert_fast(X: pd.DataFrame, Nf: int = 12,\n",
    "                      value_col: str = \"value\", period_col: str = \"period\") -> pd.DataFrame:\n",
    "    \"\"\"Vectorised expert features (fast).\"\"\"\n",
    "    # Ensure sort for FFT block splits\n",
    "    if not X.index.is_monotonic_increasing:\n",
    "        X = X.sort_index()\n",
    "\n",
    "    # Whole / pre / post stats\n",
    "    whole = _agg_basic_stats(X, value_col=value_col)\n",
    "    pre   = _agg_segment_stats(X, value_col=value_col, period_col=period_col, seg=0)\n",
    "    post  = _agg_segment_stats(X, value_col=value_col, period_col=period_col, seg=1)\n",
    "\n",
    "    # Align frames and fill gaps\n",
    "    feats = whole.join(pre, how=\"outer\").join(post, how=\"outer\").fillna(0.0)\n",
    "\n",
    "    # Deltas (post - pre)\n",
    "    for k in [\"mean\",\"std\",\"median\",\"q10\",\"q90\",\"skew\",\"kurt\"]:\n",
    "        feats[f\"delta_{k}\"] = feats.get(f\"post_{k}\", 0.0) - feats.get(f\"pre_{k}\", 0.0)\n",
    "\n",
    "    # Length features + EWM tails\n",
    "    lenf = _length_feats(X, period_col=period_col)\n",
    "    ewmf = _ewm_tail_last(X, value_col=value_col, alphas=(0.1, 0.01, 0.001))\n",
    "\n",
    "    feats = feats.join(lenf, how=\"left\").join(ewmf, how=\"left\").fillna(0.0)\n",
    "\n",
    "    # FFT band powers\n",
    "    fftf = _fft_band_powers_fast(X, value_col=value_col, Nf=Nf)\n",
    "    feats = feats.join(fftf, how=\"left\").fillna(0.0)\n",
    "\n",
    "    # Safety\n",
    "    feats = feats.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    return feats\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE & PCA WRAPPERS\n",
    "# -------------------------------\n",
    "def build_and_save_features(X_train: pd.DataFrame, X_test: pd.DataFrame,\n",
    "                            Nf: int = 12,\n",
    "                            out_dir: Path = RESOURCES_DIR) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    Z_train = f_x2z_expert_fast(X_train, Nf=Nf)\n",
    "    Z_test  = f_x2z_expert_fast(X_test,  Nf=Nf)\n",
    "\n",
    "    Z_train_path = out_dir / \"Z_train.parquet\"\n",
    "    Z_test_path  = out_dir / \"Z_test.parquet\"\n",
    "    Z_train.to_parquet(Z_train_path)\n",
    "    Z_test.to_parquet(Z_test_path)\n",
    "    print(f\"[features] saved {Z_train.shape} -> {Z_train_path}\")\n",
    "    print(f\"[features] saved {Z_test.shape}  -> {Z_test_path}\")\n",
    "    return Z_train, Z_test\n",
    "\n",
    "def fit_pca_and_transform(Z_train: pd.DataFrame, Z_test: pd.DataFrame,\n",
    "                          n_components: int | float = 0.99,\n",
    "                          out_dir: Path = RESOURCES_DIR):\n",
    "    \"\"\"\n",
    "    Fit PCA on train (n_components as int or variance fraction), transform both,\n",
    "    save PCA and transformed matrices.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components, svd_solver=\"auto\", random_state=42)\n",
    "    Ztr_pca = pca.fit_transform(Z_train.values)\n",
    "    Zte_pca = pca.transform(Z_test.values)\n",
    "\n",
    "    Ztr_pca_df = pd.DataFrame(Ztr_pca, index=Z_train.index,\n",
    "                              columns=[f\"pc_{i}\" for i in range(Ztr_pca.shape[1])])\n",
    "    Zte_pca_df = pd.DataFrame(Zte_pca, index=Z_test.index,\n",
    "                              columns=[f\"pc_{i}\" for i in range(Zte_pca.shape[1])])\n",
    "\n",
    "    joblib.dump(pca, out_dir / \"pca.joblib\")\n",
    "    Ztr_pca_df.to_parquet(out_dir / \"Z_train_pca.parquet\")\n",
    "    Zte_pca_df.to_parquet(out_dir / \"Z_test_pca.parquet\")\n",
    "    print(f\"[pca] kept {Ztr_pca.shape[1]} components; saved PCA + transformed matrices.\")\n",
    "    return Ztr_pca_df, Zte_pca_df"
   ],
   "id": "f57d65ca9f14cd75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "938dead3f4308387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "\n",
    "RESOURCES_DIR = Path(\"../resources\")\n",
    "RESOURCES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "        # import data\n",
    "        DATA_DIR = Path(\"../data\")\n",
    "        X_train = pd.read_parquet(DATA_DIR / \"X_train.parquet\")\n",
    "        X_test = pd.read_parquet(DATA_DIR / \"X_test.reduced.parquet\")\n",
    "\n",
    "        y_train = pd.read_parquet(DATA_DIR / \"y_train.parquet\").squeeze()\n",
    "        y_test = pd.read_parquet(DATA_DIR / \"y_test.reduced.parquet\").squeeze()\n",
    "\n",
    "        # after you loaded X_train, X_test\n",
    "        Z_train, Z_test = build_and_save_features(X_train, X_test, Nf=12)\n",
    "\n",
    "        # OPTIONAL: PCA to denoise / compress (keeps 99% variance)\n",
    "        Z_train_pca, Z_test_pca = fit_pca_and_transform(Z_train, Z_test, n_components=0.99)"
   ],
   "id": "577c64ef7717fc8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
