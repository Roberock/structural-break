{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Structural Break Detection â€“ Baseline with Feature Extraction & Gradient Boosting\n",
    "\n",
    "This notebook develops a **first strong baseline** for the CrunchDAO Structural Break challenge:\n",
    "\n",
    "1. Load raw training data (variable-length sequences).\n",
    "2. Extract **summary statistics features** around the break boundary.\n",
    "3. Save the extracted features for reuse.\n",
    "4. Train a **HistGradientBoostingClassifier** with cross-validation & hyperparameter tuning.\n",
    "5. Evaluate on hold-out set, report **ROC-AUC**.\n",
    "6. Save model, CV results, and prediction probabilities.\n",
    "\n"
   ],
   "id": "8d48f7f390ba5ff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:27:29.409307Z",
     "start_time": "2025-08-21T12:27:29.397361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import joblib\n",
    "\n",
    "# --- Rich features: robust stats + higher moments + FFT + wavelets ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from numpy.fft import rfft\n",
    "import pywt  # pip install PyWavelets\n"
   ],
   "id": "a12bdd7ae3fcd54c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:27:30.538019Z",
     "start_time": "2025-08-21T12:27:30.513730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _stats_robust(x):\n",
    "    if x.size == 0:\n",
    "        return {}\n",
    "    q5, q10, q25, q50, q75, q90, q95 = np.percentile(x, [5, 10, 25, 50, 75, 90, 95])\n",
    "    mad = np.median(np.abs(x - q50))  # Median Absolute Deviation\n",
    "    return {\n",
    "        \"mean\": np.mean(x), \"std\": np.std(x),\n",
    "        \"min\": np.min(x),  \"max\": np.max(x),  \"median\": q50,\n",
    "        \"q5\": q5, \"q10\": q10, \"q25\": q25, \"q75\": q75, \"q90\": q90, \"q95\": q95,\n",
    "        \"mad\": mad, \"skew\": skew(x, bias=False) if x.size > 2 else 0.0, \"kurt\": kurtosis(x, fisher=True, bias=False) if x.size > 3 else 0.0,\n",
    "        \"rms\": np.sqrt(np.mean(x**2)),  \"ptp\": np.ptp(x),  # peak-to-peak\n",
    "    }\n",
    "\n",
    "def _fft_energy_bands(x, n_bands=5):\n",
    "    \"\"\"Simple relative-band energies from power spectrum.\"\"\"\n",
    "    if x.size < 8:\n",
    "        return {f\"fft_band_{i}\": 0.0 for i in range(n_bands)}\n",
    "    spec = np.abs(rfft(x - np.mean(x)))**2\n",
    "    spec = spec[1:]  # drop DC for energy ratios\n",
    "    if spec.size == 0:\n",
    "        return {f\"fft_band_{i}\": 0.0 for i in range(n_bands)}\n",
    "    # split into equal bands\n",
    "    bands = np.array_split(spec, n_bands)\n",
    "    energies = np.array([b.sum() for b in bands], dtype=float)\n",
    "    tot = energies.sum() + 1e-12\n",
    "    return {f\"fft_band_{i}\": float(e/tot) for i, e in enumerate(energies)}\n",
    "\n",
    "def _wavelet_energies(x, wavelet=\"db4\", level=None):\n",
    "    \"\"\"Wavelet packet-ish: energy per scale from DWT coefficients.\"\"\"\n",
    "    if x.size < 8:\n",
    "        return {}\n",
    "    coeffs = pywt.wavedec(x - np.mean(x), wavelet=wavelet, level=level)\n",
    "    energies = [np.sum(c**2) for c in coeffs]  # [cA_L, cD_L, ..., cD1]\n",
    "    tot = np.sum(energies) + 1e-12\n",
    "    out = {\"wl_cA\": float(energies[0]/tot)}\n",
    "    for i, e in enumerate(energies[1:], start=1):\n",
    "        out[f\"wl_cD_{i}\"] = float(e/tot)\n",
    "    return out\n",
    "\n",
    "def _segment_features(x):\n",
    "    \"\"\"Compose robust stats + spectral features for one segment.\"\"\"\n",
    "    f = {}\n",
    "    f.update(_stats_robust(x))\n",
    "    f.update(_fft_energy_bands(x, n_bands=5))\n",
    "    f.update(_wavelet_energies(x))\n",
    "    return f\n",
    "\n",
    "def extract_features_rich(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Per-id features using value and period columns.\"\"\"\n",
    "    feats = []\n",
    "    print_count = 0\n",
    "    for id_, g in X.groupby(level=\"id\"):\n",
    "\n",
    "        if print_count == 1_000:\n",
    "            print('Progress report...extracting features from id: {}'.format(id_))\n",
    "            print_count = 0\n",
    "        print_count += 1\n",
    "\n",
    "        v = g[\"value\"].values.astype(float)\n",
    "        pre = g.loc[g[\"period\"] == 0, \"value\"].values.astype(float)\n",
    "        post = g.loc[g[\"period\"] == 1, \"value\"].values.astype(float)\n",
    "\n",
    "        d = {\"id\": id_}\n",
    "        # global\n",
    "        d.update({f\"g_{k}\": v for k, v in _segment_features(v).items()})\n",
    "\n",
    "        # pre/post\n",
    "        d.update({f\"pre_{k}\": v for k, v in _segment_features(pre).items()})\n",
    "        d.update({f\"post_{k}\": v for k, v in _segment_features(post).items()})\n",
    "\n",
    "        # deltas (post - pre) for key stats\n",
    "        for k in [\"mean\", \"std\", \"median\", \"mad\", \"skew\", \"kurt\", \"rms\"]:\n",
    "            d[f\"delta_{k}\"] = (d.get(f\"post_{k}\", 0.0) - d.get(f\"pre_{k}\", 0.0))\n",
    "\n",
    "        # counts & ratio\n",
    "        d[\"len_total\"] = int(v.size)\n",
    "        d[\"n_pre\"] = int(pre.size)\n",
    "        d[\"n_post\"] = int(post.size)\n",
    "        d[\"ratio_post_pre\"] = float(d[\"n_post\"]/(d[\"n_pre\"]+1e-6))\n",
    "        feats.append(d)\n",
    "\n",
    "    df = pd.DataFrame(feats).set_index(\"id\")\n",
    "    return df.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n"
   ],
   "id": "9e8bd60caf3a6b2a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:28:14.359954Z",
     "start_time": "2025-08-21T12:28:14.350951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RESOURCES_DIR = Path(\"../resources\")\n",
    "RESOURCES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def train_cv_tuned_model(X: pd.DataFrame, y: pd.Series):\n",
    "    \"\"\"\n",
    "    Tune HGB hyperparameters with StratifiedKFold on ROC-AUC, then refit on all data.\n",
    "    Returns (best_estimator, best_params, cv_results_df).\n",
    "    \"\"\"\n",
    "    est = HistGradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        early_stopping=True,          # quick regularisation\n",
    "        validation_fraction=0.1,\n",
    "    )\n",
    "\n",
    "    # A compact grid for ROC-AUC .... expand if time allows\n",
    "    param_grid = {\n",
    "        \"learning_rate\": [0.03, 0.06],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"max_iter\": [200, 400],\n",
    "        \"min_samples_leaf\": [10, 25, 100],\n",
    "        \"l2_regularization\": [0.0, 1e-2, 1e-1],\n",
    "        \"max_bins\": [255],            # default is strong; tune if you wish\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    sample_weight = compute_sample_weight(\"balanced\", y) # handle possible class imbalance\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator=est,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=3,\n",
    "        refit=True,   # refit on full data with best params automatically\n",
    "    )\n",
    "\n",
    "    gs.fit(X, y, sample_weight=sample_weight)\n",
    "    best_est = gs.best_estimator_\n",
    "    best_params = gs.best_params_\n",
    "    cv_results = pd.DataFrame(gs.cv_results_).sort_values(\"rank_test_score\")\n",
    "\n",
    "    print(\"\\n[CV] Best ROC-AUC (mean cv):\", gs.best_score_)\n",
    "    print(\"[CV] Best params:\", best_params)\n",
    "    return best_est, best_params, cv_results\n",
    "\n",
    "\n",
    "def evaluate_on_holdout(model, X_test: pd.DataFrame, y_test: pd.Series):\n",
    "    \"\"\"Report holdout ROC-AUC and return probability DataFrame [P(y=0), P(y=1)].\"\"\"\n",
    "    proba = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, proba[:, 1])\n",
    "    print(\"[Holdout] ROC-AUC:\", auc)\n",
    "    proba_df = pd.DataFrame(\n",
    "        proba, index=X_test.index, columns=[\"P(y=0)\", \"P(y=1)\"]\n",
    "    )\n",
    "    return auc, proba_df\n",
    "\n"
   ],
   "id": "34d56068aef06788",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:28:36.668421Z",
     "start_time": "2025-08-21T12:28:22.231151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==== end-to-end ====\n",
    "# 0) Load data\n",
    "from preprocess import load_data\n",
    "X_raw, X_test_raw, y_train, y_test = load_data('../data')\n",
    "\n",
    "# 1) extract features\n",
    "FEATURE_DIR = Path(\"../data/feature/extraction\")\n",
    "FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ],
   "id": "8af3bcf5d28194e2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:37:43.357472Z",
     "start_time": "2025-08-21T12:37:43.264569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    X_train_moments = pd.read_parquet(FEATURE_DIR / \"X_train_moments.parquet\")\n",
    "    X_test_moments = pd.read_parquet(FEATURE_DIR / \"X_test_moments.parquet\")\n",
    "    X_test_moments = X_test_moments[X_train_moments.columns]\n",
    "except:\n",
    "    X_train_moments = extract_features_rich(X_raw).fillna(0.0)\n",
    "    X_test_moments  = extract_features_rich(X_test_raw).fillna(0.0)\n",
    "    X_test_moments = X_test_moments[X_train_moments.columns]\n",
    "    X_train_moments.to_parquet(FEATURE_DIR / \"X_train_moments.parquet\")\n",
    "    X_test_moments.to_parquet(FEATURE_DIR / \"X_test_moments.parquet\")"
   ],
   "id": "ed439216351ac199",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:57:02.782641Z",
     "start_time": "2025-08-21T12:57:02.777115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(X_train, y_train, l2_regularization=0.0, max_leaf_nodes=3.1):\n",
    "    model = HistGradientBoostingClassifier(random_state=42, l2_regularization=l2_regularization, max_leaf_nodes=max_leaf_nodes)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate(model, X_val, y_val):\n",
    "    y_pred_proba = model.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    print(\"Validation ROC AUC:\", auc)\n",
    "    return auc\n"
   ],
   "id": "f9baf78a93acb77a",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:02:51.958585Z",
     "start_time": "2025-08-21T13:02:47.431650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "auc = evaluate(train_model(X_train_moments, y_train, 0.0, 31), X_test_moments, y_test)\n",
    "auc = evaluate(train_model(X_train_moments, y_train, 1e-2, 100), X_test_moments, y_test)\n",
    "model = train_model(X_train_moments, y_train, 1e-2, 100)"
   ],
   "id": "146875472d54b3cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.680281690140845\n",
      "Validation ROC AUC: 0.7450704225352113\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:41:49.943907Z",
     "start_time": "2025-08-21T12:41:49.925737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    model = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        HistGradientBoostingClassifier(random_state=42)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def tune_model(X_train, y_train):\n",
    "    param_grid = {\n",
    "        \"histgradientboostingclassifier__max_depth\": [3, 5, 7],\n",
    "        \"histgradientboostingclassifier__learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"histgradientboostingclassifier__max_iter\": [100, 200, 500]\n",
    "    }\n",
    "    model = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        HistGradientBoostingClassifier(random_state=42)\n",
    "    )\n",
    "    grid = GridSearchCV(model, param_grid, scoring=\"roc_auc\", cv=5, n_jobs=-1, verbose=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_, grid.best_params_\n",
    "\n",
    "\n",
    "def evaluate(model, X_val, y_val):\n",
    "    y_pred_proba = model.predict_proba(X_val)[:,1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_val, y_pred_proba)\n",
    "    brier = brier_score_loss(y_val, y_pred_proba)\n",
    "    print(f\"ROC AUC: {auc:.4f}, PR AUC: {pr_auc:.4f}, Brier: {brier:.4f}\")\n",
    "    return {\"roc_auc\": auc, \"pr_auc\": pr_auc, \"brier\": brier}\n",
    "\n",
    "\n",
    "def cross_val_auc(model, X, y, cv=5):\n",
    "    return cross_val_score(model, X, y, cv=cv, scoring=\"roc_auc\").mean()"
   ],
   "id": "a9f063f8bbc27ac7",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:45:05.045381Z",
     "start_time": "2025-08-21T12:41:51.788666Z"
    }
   },
   "cell_type": "code",
   "source": "model, best_param = tune_model(X_train_moments, y_train)",
   "id": "d9f545afc885406f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:47:21.508195Z",
     "start_time": "2025-08-21T12:47:18.787593Z"
    }
   },
   "cell_type": "code",
   "source": "cross_val_auc(model, X_test_moments, y_test, cv=5)",
   "id": "e1b273f2e7a614d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4531746031746032)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 2) CV + tuning, refit on all training\n",
    "model, best_params, cv_results = train_cv_tuned_model(X_train_moments, y_train)\n"
   ],
   "id": "5604fb618daa618b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 3) Evaluate on your provided X_test / y_test\n",
    "holdout_auc, proba_test = evaluate_on_holdout(model, X_test_moments, y_test)\n",
    "\n",
    "# 4) Persist artifacts\n",
    "joblib.dump(model, RESOURCES_DIR / \"hgb_structbreak_model.joblib\")\n",
    "cv_results.to_csv(RESOURCES_DIR / \"cv_results.csv\", index=False)\n",
    "proba_test.to_parquet(RESOURCES_DIR / \"proba_test.parquet\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"-\", RESOURCES_DIR / \"hgb_structbreak_model.joblib\")\n",
    "print(\"-\", RESOURCES_DIR / \"cv_results.csv\")\n",
    "print(\"-\", RESOURCES_DIR / \"proba_test.parquet\")"
   ],
   "id": "2cbb6288094f7495"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T10:14:26.702572Z",
     "start_time": "2025-08-21T10:14:26.441726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- ROC AUC plot on the hold-out set ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get ground truth (aligned to X_test's index if needed)\n",
    "y_true = y_test.loc[X_test_moments.index] if isinstance(y_test, (pd.Series, pd.DataFrame)) else y_test\n",
    "y_true = y_true.squeeze().astype(int).values  # ensure 1D int array\n",
    "\n",
    "# Extract positive-class scores from your probabilities\n",
    "# (works whether you kept numpy array from predict_proba or a DataFrame with \"P(y=1)\")\n",
    "if isinstance(proba_test, np.ndarray):\n",
    "    y_score = proba_test[:, 1]\n",
    "else:\n",
    "    y_score = pd.Series(proba_test).values  # adjust if you stored as a Series\n",
    "\n",
    "# Compute ROC curve & AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Optional: best threshold by Youden's J statistic\n",
    "j_idx = np.argmax(tpr - fpr)\n",
    "best_thr = thresholds[j_idx]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(fpr, tpr, lw=2, label=f\"ROC AUC = {roc_auc:.4f}\")\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=1)\n",
    "ax.scatter(fpr[j_idx], tpr[j_idx], s=30, label=f\"Best thr â‰ˆ {best_thr:.4f}\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC Curve (hold-out)\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.6f}\")\n",
    "print(f\"Best threshold (Youden J): {best_thr:.6f}\")\n"
   ],
   "id": "327274bfc6c7ebed",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (101, 2) instead",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m     y_score = proba_test[:, \u001B[32m1\u001B[39m]\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     y_score = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproba_test\u001B[49m\u001B[43m)\u001B[49m.values  \u001B[38;5;66;03m# adjust if you stored as a Series\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Compute ROC curve & AUC\u001B[39;00m\n\u001B[32m     19\u001B[39m fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\structural_break\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:584\u001B[39m, in \u001B[36mSeries.__init__\u001B[39m\u001B[34m(self, data, index, dtype, name, copy, fastpath)\u001B[39m\n\u001B[32m    582\u001B[39m         data = data.copy()\n\u001B[32m    583\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m584\u001B[39m     data = \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    586\u001B[39m     manager = _get_option(\u001B[33m\"\u001B[39m\u001B[33mmode.data_manager\u001B[39m\u001B[33m\"\u001B[39m, silent=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    587\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m manager == \u001B[33m\"\u001B[39m\u001B[33mblock\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\structural_break\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:630\u001B[39m, in \u001B[36msanitize_array\u001B[39m\u001B[34m(data, index, dtype, copy, allow_2d)\u001B[39m\n\u001B[32m    628\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    629\u001B[39m         data = np.array(data, copy=copy)\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    633\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    635\u001B[39m \u001B[43m        \u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    636\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    639\u001B[39m     _sanitize_non_ordered(data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\structural_break\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:656\u001B[39m, in \u001B[36msanitize_array\u001B[39m\u001B[34m(data, index, dtype, copy, allow_2d)\u001B[39m\n\u001B[32m    653\u001B[39m             subarr = cast(np.ndarray, subarr)\n\u001B[32m    654\u001B[39m             subarr = maybe_infer_to_datetimelike(subarr)\n\u001B[32m--> \u001B[39m\u001B[32m656\u001B[39m subarr = \u001B[43m_sanitize_ndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    658\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subarr, np.ndarray):\n\u001B[32m    659\u001B[39m     \u001B[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001B[39;00m\n\u001B[32m    660\u001B[39m     dtype = cast(np.dtype, dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\structural_break\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:715\u001B[39m, in \u001B[36m_sanitize_ndim\u001B[39m\u001B[34m(result, data, dtype, index, allow_2d)\u001B[39m\n\u001B[32m    713\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m allow_2d:\n\u001B[32m    714\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m--> \u001B[39m\u001B[32m715\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    716\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mData must be 1-dimensional, got ndarray of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m instead\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    717\u001B[39m     )\n\u001B[32m    718\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_object_dtype(dtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, ExtensionDtype):\n\u001B[32m    719\u001B[39m     \u001B[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001B[39;00m\n\u001B[32m    721\u001B[39m     result = com.asarray_tuplesafe(data, dtype=np.dtype(\u001B[33m\"\u001B[39m\u001B[33mobject\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[31mValueError\u001B[39m: Data must be 1-dimensional, got ndarray of shape (101, 2) instead"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3fded0d21cca7106"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "def fit_ensemble_avg(X, y, seeds=(42, 123, 2025)):\n",
    "    members = []\n",
    "    for s in seeds:\n",
    "        clf = HistGradientBoostingClassifier(\n",
    "            random_state=s,\n",
    "            max_iter=600, max_depth=5, learning_rate=0.06,\n",
    "            min_samples_leaf=25, l2_regularization=1e-2, early_stopping=True\n",
    "        )\n",
    "        clf.fit(X, y)\n",
    "        members.append(clf)\n",
    "    return members\n",
    "\n",
    "def predict_ensemble_avg(models, X):\n",
    "    # average positive-class probability\n",
    "    probs = [m.predict_proba(X)[:, 1] for m in models]\n",
    "    return np.mean(probs, axis=0)\n",
    "\n",
    "# Train\n",
    "ens = fit_ensemble_avg(X_train_moments, y_train)\n",
    "# Predict\n",
    "proba_test_ens = predict_ensemble_avg(ens, X_test_moments)\n"
   ],
   "id": "169783a3aa70be7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "base_learners = [\n",
    "    (\"hgb1\", HistGradientBoostingClassifier(random_state=42, max_iter=800, max_depth=5, learning_rate=0.06)),\n",
    "    (\"hgb2\", HistGradientBoostingClassifier(random_state=123, max_iter=400, max_depth=7, learning_rate=0.03)),\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=LogisticRegression(max_iter=2000),\n",
    "    stack_method=\"predict_proba\",\n",
    "    passthrough=False,  # set True if you want to pass original features to meta-learner\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack.fit(X_train_moments, y_train)\n",
    "proba_test_stack = stack.predict_proba(X_test_moments)[:, 1]\n"
   ],
   "id": "16b3a98daf62a7e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pip install tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "def build_sequence_arrays(X: pd.DataFrame, ids=None, max_len=None):\n",
    "    \"\"\"Return padded [num_ids, max_len, 2] (value, period), lengths, id_index.\"\"\"\n",
    "    if ids is None:\n",
    "        ids = X.index.get_level_values(\"id\").unique().tolist()\n",
    "    seqs = []\n",
    "    for i in ids:\n",
    "        g = X.xs(i, level=\"id\")\n",
    "        seqs.append(np.stack([g[\"value\"].values.astype(\"float32\"),\n",
    "                              g[\"period\"].values.astype(\"float32\")], axis=1))\n",
    "    lengths = [len(s) for s in seqs]\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "    # pad with zeros at the end\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, dtype=\"float32\", padding=\"post\", truncating=\"post\")\n",
    "    return np.asarray(seqs_pad), np.asarray(lengths), ids\n",
    "\n",
    "def build_gru_model(input_len, input_dim=2, rnn_units=64, dropout=0.2):\n",
    "    inp = layers.Input(shape=(input_len, input_dim))\n",
    "    x = layers.Masking(mask_value=0.0)(inp)\n",
    "    x = layers.Bidirectional(layers.GRU(rnn_units, return_sequences=False, dropout=dropout))(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3),\n",
    "                  loss=\"binary_crossentropy\", metrics=[\"AUC\"])\n",
    "    return model\n",
    "\n",
    "# Build arrays\n",
    "X_train_seq, train_len, train_ids = build_sequence_arrays(X_raw)\n",
    "X_test_seq,  test_len,  test_ids  = build_sequence_arrays(X_test_raw, max_len=X_train_seq.shape[1])\n",
    "\n",
    "# Targets aligned to train_ids\n",
    "y_train_arr = y_train.loc[train_ids].astype(int).values\n",
    "\n",
    "# Model\n",
    "deep_model = build_gru_model(X_train_seq.shape[1], input_dim=2, rnn_units=64)\n",
    "deep_model.fit(X_train_seq, y_train_arr, epochs=8, batch_size=64, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Probabilities\n",
    "proba_test_deep = deep_model.predict(X_test_seq, verbose=0).squeeze()  # P(y=1)\n"
   ],
   "id": "1d8740a3558b454e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# e.g., calibrate the best HGB\n",
    "best_hgb = HistGradientBoostingClassifier(\n",
    "    random_state=42, max_iter=800, max_depth=5, learning_rate=0.06,\n",
    "    min_samples_leaf=25, l2_regularization=1e-2\n",
    ").fit(X_train_moments, y_train)\n",
    "\n",
    "# method='isotonic' (non-parametric) or 'sigmoid' (Platt scaling)\n",
    "cal = CalibratedClassifierCV(best_hgb, method=\"isotonic\", cv=5)\n",
    "cal.fit(X_train_moments, y_train)\n",
    "proba_test_cal = cal.predict_proba(X_test_moments)[:, 1]\n"
   ],
   "id": "4e39bcfe6c5192f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
